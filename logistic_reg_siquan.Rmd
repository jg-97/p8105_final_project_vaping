---
title: "logistic_reg_Siquan"
author: "Siquan"
date: "12/2/2019"
output: github_document
---
```{r setup, include = FALSE}
library(tidyverse)
library(modelr)
library(caret)
library(pscl)
```

```{r Data Import}
# Loading the cleaned data
load("./data/nyc_data.RData")
```

#I decide to use LASSO to automatically help me choose the important covariates in the logistic regression model based on cross-validation error

```{r}
# Extracting necessary variables (in the following analysis, we only focus on the variables extracted by Daniela in the original'Logistics Regressin.Rmd')
df = df_total %>% 
  filter(year == 2017| year ==2015) %>% # Basing model on data from both 2015 and 2017
  select(
    year,
    # binary respone
    current_vaping,
    # possible predictor
    texting_and_driving,
    carring_weapon,
    sad_hopeless,
    attempted_suicide,
    injurious_suicide_attempt,
    safety_concerns_at_school,
    threatened_at_school,
    physical_fighting,
    bullying_at_school,
    bullying_electronically,
    sex,
    age,
    race7,
    borough)

summary(df)
str(df)
#Remove all observations with any missing value
df<-na.omit(df)
#Split data into training and test data set
sample_size = floor(0.8*nrow(df))
set.seed(8105)
picked = sample(seq_len(nrow(df)),size = sample_size)
data_tr =df[picked,]
data_te =df[-picked,]
x_tr <- data_tr[,c(1,3:16)]
y_tr <- data_tr[,2]
x_te <- data_te[,c(1,3:16)]
y_te <- data_te[,2]
#use LASSO since we have so many predictors and automatically let R help us select the important variables
colnames(data_tr)
library(glmnet)
library(gmodels)
#create train set input
Training_x <-  model.matrix( ~ year + carring_weapon +texting_and_driving +carring_weapon+sad_hopeless+attempted_suicide+injurious_suicide_attempt+ safety_concerns_at_school+threatened_at_school+physical_fighting+bullying_at_school+bullying_electronically+sex+age+race7+borough -1, data_tr)
#Use LASSO to do auto variable selection accoridng to cross-validation error
cv.lasso <- cv.glmnet(Training_x, as.matrix(as.numeric(unlist(y_tr))-1), alpha = 1, family = "binomial")
model_lasso <- glmnet(Training_x, as.matrix(as.numeric(unlist(y_tr))-1), alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
coef(model_lasso)
#Training set prediction accuracy
model_lasso_train<- round(predict(model_lasso, Training_x, type = "response"))
CrossTable(x = as.numeric(unlist(y_tr))-1,y = model_lasso_train, prop.r = F, prop.c = F, prop.chisq = F)
mean(as.numeric(unlist(y_tr))-1 ==model_lasso_train)
#Test set prediction accuracy
Testing_x <-  model.matrix( ~ year + carring_weapon +texting_and_driving +carring_weapon+sad_hopeless+attempted_suicide+injurious_suicide_attempt+ safety_concerns_at_school+threatened_at_school+physical_fighting+bullying_at_school+bullying_electronically+sex+age+race7+borough -1, data_te)

model_lasso_test<- round(predict(model_lasso, Testing_x, type = "response"))
CrossTable(x = as.numeric(unlist(y_te))-1,y = model_lasso_test, prop.r = F, prop.c = F, prop.chisq = F)
mean(as.numeric(unlist(y_te))-1 ==model_lasso_test)
#Use prediction accuracy as the evaluation matrix in binary classification task, train set prediction accuracy  0.7879127 and test set prediction accuracy 0.7785235. SInce the training set and test set accuracy are pretty close, there is no overfitting.

```
#Note that we could do the same procedure with the imputed data and see whether the prediction accuracy has been imporved.

#Also note that we could add more potential covariates into the model and see whether the prediction accuracy has been imporved.

#Compare with the original model
```{r}
fit_logistic = glm(current_vaping ~ sad_hopeless + attempted_suicide  + safety_concerns_at_school + threatened_at_school + physical_fighting + bullying_electronically + carring_weapon, data = data_tr, family = binomial())

fit_logistics_predict = round(predict(fit_logistic, data_te, type = "response"))
CrossTable(x = as.numeric(unlist(y_te))-1,y = fit_logistics_predict, prop.r = F, prop.c = F, prop.chisq = F)
mean(as.numeric(unlist(y_te))-1 ==fit_logistics_predict)
```
The original model has a test prediction accuracy of 0.7695749, which is a little bit lower than the LASSO model, but I think both of these two models are okay and we could then try to model with imputede data and even adding some more covariates we are interested in to the model.
