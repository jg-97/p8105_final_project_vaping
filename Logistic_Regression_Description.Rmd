---
output: 
  html_document:
    theme: journal
---
```{r include=FALSE}
library(tidyverse)
library(glmnet)
###This package provide glm like interface for glmnet
library(glmnetUtils)
set.seed(8105)
```


<br> <br>

# Logistic Regression Model

<br>


Since we were investigating the prevelance and trend in vaping among NYC's youth, we were interested in predicting **current_vaping**. We began our model building process considering 17 predictors, and employed three different methods:


<br> <br>

## Manuel Method

<br>

First, we attempted to find a predictive model by using a variation on **stepwise/automatic procedures** (by hand). We utilized p-values and prediction accuracy as our guidance for which predictors to choose, starting with the full model. 

In the end, this process came up with the following possible model:

<br>
**Model 1**

current_vaping ~ sad_hopeless + attempted_suicide + safety_concerns_at_school + illegal_injected_drug_use + physical_fighting + bullying_electronically + carring_weapon + sex_before_13

<br> <br>

## Step AIC Model

<br>

Considering the large number of predictor candidates in our model, we decided to take advantage of the existing modern computational power and use the stepwise regression method to come up with a model. We used the AIC criterion, a goodness of fit measure that helps to avoid overfitting. It also circumvents the big p value problem introduced by our potentially highly correlated predictor candidates.
The actual function used is the StepAIC function from the MASS package.


The formula generated by the function is as follows:

<br>
**Model 2**

current_vaping ~ carring_weapon + sad_hopeless + attempted_suicide + 
   safety_concerns_at_school + physical_fighting + bullying_electronically + 
    age + race7 + illegal_injected_drug_use + sexual_contact_2


<br> <br>

## LASSO

<br>

The motivation behind using the LASSO method is that we have a lot of potential predictors available and we cannot (and do not want to) to do an exhaustive search manually. LASSO is a shinkage method that avoids overfitting and help with variable selection. These advantages make LASSO one of the most popular methods in regression problem settings. In our study we chose the penalty parameter lamda based on the cross-validation error. Then we used the optimal lamda to rerun the LASSO again to get our final model.


In the end the final model from LASSO is the following:

<br>
**Model 3:** 

current_vaping ~ age + sex + race7 + sad_hopeless + attempted_suicide + injurious_suicide_attempt + safety_concerns_at_school + physical_fighting + bullying_electronically + illegal_injected_drug_use + carring_weapon + sex_before_13 + current_sexual_activity

<br>
The LASSO model has tunning paramter lamda equals 0.005 and the model contains more covariates than the above two models since LASSO putting shringkage on the coefficient of each covariate and thus will include more covariates (remember that LASSO will automatically do the variable selection).


<br>


## Model Selection 
<br>

### Picking the "best" Model
<br>

At this point we had three predictive models. In order to decide which of them is the "best" one, we employed the cross-validation prediction accuracy as our criterion. The prediction accuracy is calculated as the proportion of correct predictions made by the model.

To perform the cross-validation in a compact and well-integrated manner, we coded our model selection process as robust functions that can be mapped to a modlr cv object by purrr to streamline the cross-validation process. We conducted a 5 fold 10 times CV on the three models.


```{r include = FALSE}
## function for stepAIC method
log_reg_auto = function(cv_df){
  cv_df %>% mutate(
###Automodel selection with AIC as criterial
         automodel  = map(train, ~glm(current_vaping ~., data = .x, family = binomial(),na.action = na.exclude)%>%MASS::stepAIC(trace = FALSE))
 ) }

###function for lasso selection method
lasso = function(df){
  lasso_temp = cv.glmnet(current_vaping ~., data = df, family = "binomial")
glmnet(current_vaping ~., data = df, family = "binomial",lambda = lasso_temp$lambda.min)
}
### formular for manual selection
manual_formular = current_vaping ~ sad_hopeless + attempted_suicide + safety_concerns_at_school + 
    threatened_at_school + physical_fighting + bullying_electronically + 
    carring_weapon

###find prediction accuracy
accy = function(model,data){
  Temp = predict(model, data, type = "response") %>% round()
1 - mean(Temp - (as.numeric(pull(as.tibble(data),current_vaping)) - 1))
}
```

```{r include=FALSE}

load("./data/nyc_data.RData")

# Creating the dataset we will use to build the model
df = df_total %>% 
  ### Select only the vairbale candidates from the data set 
  select(
    # binary respone
    current_vaping,
    # possible predictor
    borough,
    year,
    age,
    sex,
    race7,
    sad_hopeless,
    attempted_suicide,
    injurious_suicide_attempt,
    considered_suicide,
    safety_concerns_at_school,
    threatened_at_school,
    physical_fighting,
    bullying_at_school,
    bullying_electronically,
  illegal_injected_drug_use,
    carring_weapon,
    sex_before_13,
    current_sexual_activity)


df15 = df %>% filter(year == 2015) %>% 
  ###Wanna do complete analysis
  drop_na()

df17 = df %>% filter(year == 2017) %>% 
  ###Wanna do complete analysis
  drop_na()

```



```{r eval=FALSE, include=FALSE}
cv_df=
  ## Making the CV_df by modelr, for 5 folds and 10 times.
  crossv_mc(df15,n=10)


cv_df = cv_df%>%
##stepAIC
  log_reg_auto()%>%
  ## Manual model
  mutate(manual_model = map(train, ~glm(manual_formular, data = .x, family = binomial,na.action = na.exclude)))%>%
  ## Lasso model
 mutate(lasso_model = map(train,~lasso(.x)))

###Prediction Accuracy computation
cv_df = cv_df%>%
  mutate(
        accuracy_automodel= map2_dbl(.x = automodel,.y = test, ~accy(data =.y,model =.x)),
        accuracy_manualmodel= map2_dbl(.x = manual_model,.y = test, ~accy(data =.y,model =.x)),
        accuracy_lasso= map2_dbl(.x = lasso_model,.y = test, ~accy(data =.y,model =.x))
        )


###Graph accuracy distribution
cv_df%>%select(starts_with("accuracy")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "Accuracy",
    names_prefix = "accuracy_") %>% 
  mutate(Models = recode(model,automodel = "StepAIC Model",manualmodel = "Manual Model", lasso = "Lasso Model"))%>%
  ggplot(aes(x = Models, y = Accuracy)) + geom_violin()
```

<br>

<left><img src = "Logistic_Regression_Model_Selection_files/figure-html/CrossValidation-1.png" style = "width:60%"> </left>


According to the violin plot, which shows the distribution of prediction accuracy, the model that was generated by stepAIC (StepAIC model) has an accuracy of about 1% better than the accuracy of the model generated by lasso (Lasso model) and an accuracy of about 2% better than the model generated by the manual selection (Manual model) method. Therefore, we picked the stepAIC model as our finial model.



<br>

### Model applied to 2017 Data

<br>

Subsequently, we used all three models to predict the vaping status of teenagers in 2017. As can be seen in the table below, the StepAIC model also performed best among the three models considered, giving a more than 87% accuracy rate. 



```{r include=FALSE}
## The model generated using the full data set is recreated
Final_model =
  ## tibble
  tibble(train = list(df15),
         test = list(df17)) %>%
  ##stepAIC
  log_reg_auto() %>%
  ## Manual model
  mutate(manualmodel = map(train, ~glm(manual_formular, data = .x, family = binomial,na.action = na.exclude))) %>%
  ## Lasso model
  mutate(lassomodel = map(train,~lasso(.x)))
```

```{r include=FALSE}
Final_model %>%
  mutate(
        accuracy_automodel = map2_dbl(.x = automodel,.y = test, ~accy(data =.y,model =.x)),
        accuracy_manualmodel = map2_dbl(.x = manualmodel,.y = test, ~accy(data =.y,model =.x)),
        accuracy_lasso = map2_dbl(.x = lassomodel,.y = test, ~accy(data =.y,model =.x))
        ) %>%
  select("Accuracy StepAIC Model" = accuracy_automodel, "Accuracy Manual Model" = accuracy_manualmodel, "Accuracy Lasso Model" = accuracy_lasso) %>% round(digits = 3) %>% knitr::kable(caption = "Prediction Accuray by Model on 2017 data")
```



<br>

In conclusion, the final logistic regression model (StepAIC model) has the following formulae:

**current_vaping ~ carring_weapon + sad_hopeless + attempted_suicide + safety_concerns_at_school + physical_fighting + bullying_electronically + age + race7 + illegal_injected_drug_use + sexual_contact_2**

<br><br>