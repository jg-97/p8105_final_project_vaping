---
title: "Logistic Regression"
author: "Daniela Quigee (dq2147)"
date: "12/01/2019"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(modelr)
library(pscl)
library(caret)
library(glmnet)
library(gmodels)

###This package provide glm like interface for glmnet
library(glmnetUtils)
```

```{r Data Import}
# Loading the cleaned data
load("./data/nyc_data.RData")
fill_NA = function(vector){
    as.factor(coalesce(as.character(vector),"unknown"))
}
set.seed(8105)
```
<<<<<<< HEAD:Logistic Regression.Rmd
###Manual selection


```{r Fitting the first logistic regression model, eval=FALSE, include=FALSE}
=======

```{r}
# Extracting necessary variables
df = df_total %>% 
  filter(year == 2017) %>% # Basing model on data from 2017
  select(
    # binary respone
    current_vaping,
    # possible predictor
    texting_and_driving,
    carring_weapon,
    sad_hopeless,
    attempted_suicide,
    injurious_suicide_attempt,
    safety_concerns_at_school,
    threatened_at_school,
    physical_fighting,
    bullying_at_school,
    bullying_electronically,
    sex,
    age,
    race7,
    borough)
>>>>>>> 172ccea276d1a5737171c12a09e7fab4f69b27be:Logistic_Regression.Rmd
# Fitting a logistic regression model
fit_logistic = glm(current_vaping ~ sad_hopeless + attempted_suicide  + safety_concerns_at_school + threatened_at_school + physical_fighting + bullying_electronically + carring_weapon  , data = df, family = binomial())
fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  knitr::kable(digits = 3)
# What is the contribution of each predictor? see: https://uc-r.github.io/logistic_regression#multi
caret::varImp(fit_logistic)
# Pseudo R^2: see https://uc-r.github.io/logistic_regression#multi (Pseudo R^2 about 0.40 considered good)
<<<<<<< HEAD:Logistic Regression.Rmd


pR2(  glm(current_vaping ~ sad_hopeless + attempted_suicide  + safety_concerns_at_school + threatened_at_school + physical_fighting + bullying_electronically + carring_weapon  , data = df17, family = binomial())  )["McFadden"]

AIC(fit_logistic)

=======
pR2(  glm(current_vaping ~ sad_hopeless + attempted_suicide  + safety_concerns_at_school + threatened_at_school + physical_fighting + bullying_electronically + carring_weapon  , data = df, family = binomial())  )["McFadden"]
>>>>>>> 172ccea276d1a5737171c12a09e7fab4f69b27be:Logistic_Regression.Rmd
# Cross-Validating the model
data_train <- trainControl(method = "cv", number = 5)
model_caret <- train(
  current_vaping ~ sad_hopeless + attempted_suicide  + safety_concerns_at_school + threatened_at_school + physical_fighting + bullying_electronically + carring_weapon,
                   data = df,
                   trControl = data_train,
                   method = 'glm',
                   family = binomial(),
                   na.action = na.pass)

# Model predictions using 4 parts of the data for training 
model_caret
<<<<<<< HEAD:Logistic Regression.Rmd


manual_formular = current_vaping ~ sad_hopeless + attempted_suicide  + safety_concerns_at_school + threatened_at_school + physical_fighting + bullying_electronically + carring_weapon + year
```


## Function definition
The following code chunck define the functions that is gonna be used to generate and mapped on the cv dataframe, i.e. it build up the component of the pipeline.

```{r Function definition}

###Function that do auto log reg

## function for cv
#crossv_mc(df,n)

## function for autologreg
log_reg_auto = function(cv_df,formular = manual_formular){
  cv_df %>%mutate(
###Automodel selection with AIC as criterial
         automodel  = map(train, ~glm(current_vaping ~., data = .x, family = binomial(),na.action = na.exclude)%>%MASS::stepAIC(trace = FALSE))
 ) }


###Do lasso stat learning
lasso = function(cv_df){
  lasso_temp = cv.glmnet(current_vaping ~., data = cv_df, family = "binomial")
glmnet(current_vaping ~., data = cv_df, family = "binomial",lambda = lasso_temp$lambda.min)
}

###find prediction accuracy
accy = function(model,data){
  Temp = predict(model, data, type = "response")%>%round()
1-mean(Temp-(as.numeric(pull(as.tibble(data),current_vaping))-1))
  }
```


## data input selection:
This part control the datas that was used to feed in the pipeline


```{r Data retrieving}
# Extracting necessary variables
df = df_total%>%
  select(
    # binary respone
    current_vaping,
    # possible predictor
    carring_weapon,
    sad_hopeless,
    attempted_suicide,
    injurious_suicide_attempt,
    safety_concerns_at_school,
    threatened_at_school,
    physical_fighting,
    bullying_at_school,
    bullying_electronically,
    sex,
    age,
    race7,
    borough,
    illegal_injected_drug_use,
sex_before_13,
multiple_sex_partner,
sexual_contact_2
,year)


df1517 = df%>%filter(year == 2015|year == 2017)%>%na.omit()
df15 = df%>%filter(year == 2017)%>%na.omit()
df17 = df%>%filter(year == 2015)%>%na.omit()



```

### Actual pipeline
The following part commbine all the component of the pipelines and feed the table.


```{r implimentation}
###Set the manual selected predictor and response for the manual model
manual_formular = current_vaping ~ sad_hopeless + attempted_suicide  + safety_concerns_at_school + threatened_at_school + physical_fighting + bullying_electronically + carring_weapon + year

### run the actual pipeline
regtb_1517=
  ## CV 
  crossv_mc(df1517,n=10)%>%
  ##stepAIC
  log_reg_auto()%>%
  ## Manual model
  mutate(manualmodel = map(train, ~glm(manual_formular, data = .x, family = binomial,na.action = na.exclude)))%>%
  ## Lasso model
  mutate(lassomodel = map(train,~lasso(.x)))

###Prediction Accuracy computation
regtb_1517=regtb_1517%>%
  mutate(
        accuracy_automodel= map2_dbl(.x = automodel,.y = test, ~accy(data =.y,model =.x)),
        accuracy_manualmodel= map2_dbl(.x = manualmodel,.y = test, ~accy(data =.y,model =.x)),
        accuracy_lasso= map2_dbl(.x = lassomodel,.y = test, ~accy(data =.y,model =.x))
        )


###Graph accuracy distribution
regtb_1517%>%select(starts_with("accuracy")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "accuracy",
    names_prefix = "accuracy_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = accuracy)) + geom_violin()
```


```{r finaloutput}
###Finalize model retrived:

Final_1517=
  ## CV 
  tibble(train = list(df1517),
         test = list(df1517))%>%
  ##stepAIC
  log_reg_auto()%>%
  ## Manual model
  mutate(manualmodel = map(train, ~glm(manual_formular, data = .x, family = binomial,na.action = na.exclude)))%>%
  ## Lasso model
  mutate(lassomodel = map(train,~lasso(.x)))

Final_1517%>%
  mutate(
        accuracy_automodel= map2_dbl(.x = automodel,.y = test, ~accy(data =.y,model =.x)),
        accuracy_manualmodel= map2_dbl(.x = manualmodel,.y = test, ~accy(data =.y,model =.x)),
        accuracy_lasso= map2_dbl(.x = lassomodel,.y = test, ~accy(data =.y,model =.x))
        )

final_modle_formula = list(
Final_1517[[1,3]]$formula,
Final_1517[[1,4]]$formula,
Final_1517[[1,5]]$terms
)
final_modle_formula

  accy(Final_1517[[1,5]],Final_1517[[1,1]])
```



```{r prediction, eval=FALSE, include=FALSE}
##Accy for 2017 only data


  
  
             

type = "response"

mean(predict(Final_1517[[1,5]], df%>%filter(year == 2011)%>%na.omit(),type = "response"))
##Accy for 2015 data

##Retrocast 2013:

```




####################################################################################################################
The incoperated log reg pipeline ends here
























```{r eval=FALSE, include=FALSE}



#create train set input
Training_x <-  model.matrix( ~ year + carring_weapon +texting_and_driving +carring_weapon+sad_hopeless+attempted_suicide+injurious_suicide_attempt+ safety_concerns_at_school+threatened_at_school+physical_fighting+bullying_at_school+bullying_electronically+sex+age+race7+borough -1, data_tr)





#Training set prediction accuracy
model_lasso_train<- round(predict(model_lasso, Training_x, type = "response"))
model_lasso_train

CrossTable(x = as.numeric(unlist(y_tr))-1,y = model_lasso_train, prop.r = F, prop.c = F, prop.chisq = F)

mean(as.numeric(unlist(y_tr))-1 ==model_lasso_train)
#Test set prediction accuracy
Testing_x <-  model.matrix( ~ year + carring_weapon +texting_and_driving +carring_weapon+sad_hopeless+attempted_suicide+injurious_suicide_attempt+ safety_concerns_at_school+threatened_at_school+physical_fighting+bullying_at_school+bullying_electronically+sex+age+race7+borough -1, data_te)

model_lasso_test<- round(predict(model_lasso, Testing_x, type = "response"))
CrossTable(x = as.numeric(unlist(y_te))-1,y = model_lasso_test, prop.r = F, prop.c = F, prop.chisq = F)
mean(as.numeric(unlist(y_te))-1 ==model_lasso_test)
#Use prediction accuracy as the evaluation matrix in binary classification task, train set prediction accuracy  0.7879127 and test set prediction accuracy 0.7785235. SInce the training set and test set accuracy are pretty close, there is no overfitting.

```
#Note that we could do the same procedure with the imputed data and see whether the prediction accuracy has been imporved.

#Also note that we could add more potential covariates into the model and see whether the prediction accuracy has been imporved.

#Compare with the original model
``{r}
fit_logistic = glm(current_vaping ~ sad_hopeless + attempted_suicide  + safety_concerns_at_school + threatened_at_school + physical_fighting + bullying_electronically + carring_weapon, data = data_tr, family = binomial())

fit_logistics_predict = round(predict(fit_logistic, data_te, type = "response"))
CrossTable(x = as.numeric(unlist(y_te))-1,y = fit_logistics_predict, prop.r = F, prop.c = F, prop.chisq = F)
mean(as.numeric(unlist(y_te))-1 ==fit_logistics_predict)
```
```



=======
AIC(fit_logistic)
```
>>>>>>> 172ccea276d1a5737171c12a09e7fab4f69b27be:Logistic_Regression.Rmd
