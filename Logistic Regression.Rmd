---
title: "Logistic Regression"
author: "Daniela Quigee (dq2147)"
date: "12/01/2019"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(modelr)
library(pscl)
library(caret)
```

```{r Data Import}
# Loading the cleaned data
load("./data/nyc_data.RData")
fill_NA = function(vector){
    as.factor(coalesce(as.character(vector),"unknown"))
}
set.seed(8105)
```

```{r Function definition}
log_reg_auto = function(df ,n=100,formular = manual_formular){
  ###Remove all the columns with only one variable
  ### 8:2 the df for cv
  cv_df = crossv_mc(df,n)
### do the cv
  cv_df = 
  cv_df %>%mutate(
###Automodel selection with AIC as criterial
         automodel  = map(train, ~glm(current_vaping ~., data = .x, family = binomial(),na.action = na.exclude)%>%MASS::stepAIC(trace = FALSE))
 ) }

psr2glm=function(glmobj,family = binomial){
  L.base=
    logLik(
      glm(formula = reformulate('1',gsub( " .*$", "", deparse(glmobj$formula) )),
          data=glmobj$data,
          family = family))

  n=length(glmobj$residuals)

  L.full=logLik(glmobj)
  D.full <- -2 * L.full
  D.base <- -2 * L.base
  G2 <- -2 * (L.base - L.full)

  return(data.frame(McFadden = 1-L.full/L.base, 
                    CoxSnell = 1 - exp(-G2/n),
                    Nagelkerke = (1 - exp((D.full - D.base)/n))/(1 - exp(-D.base/n))))

}


cv_graph = function(cv_df,parameter = "McFadden_pr2"){
cv_df %>% 
  select(starts_with(parameter)) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = parameter,
    names_prefix = parameter) %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()


}




lasso = function(cv_df = cv_df){}

```

```{r Data retrieving}
# Extracting necessary variables
df = df_total%>%
  
  
  ##map_df(df_total%>%select(-current_vaping),fill_NA)%>%mutate(id = as.numeric(as.character(id)),year = as.numeric(as.character(year)),current_vaping = df_total%>%pull(current_vaping))%>%
# Basing model on data from 2017
  select(
    # binary respone
    current_vaping,
    # possible predictor
    #texting_and_driving,
    carring_weapon,
    sad_hopeless,
    attempted_suicide,
    injurious_suicide_attempt,
    safety_concerns_at_school,
    threatened_at_school,
    physical_fighting,
    bullying_at_school,
    bullying_electronically,
    sex,
    age,
    race7,
    borough,
    illegal_injected_drug_use,
sex_before_13,
multiple_sex_partner,
sexual_contact_2
,year)


df15 = df%>%filter(year == 2015)%>%select(-year)%>%na.omit()
df17 = df%>%filter(year == 2017)%>%select(-year)%>%na.omit()
df_ftred15 = Filter(function(x) length(unique(x))!=1, df15)
df_ftred17 = Filter(function(x) length(unique(x))!=1, df17)

```

```{r finalize}
formular = current_vaping ~ sad_hopeless + attempted_suicide  + safety_concerns_at_school + threatened_at_school + physical_fighting + bullying_electronically + carring_weapon

###Fit the model with auto and manual chosed distribution
regtb_17 = log_reg_auto(df17,n=5)
regtb_15 = log_reg_auto(df15,n=5)



regtb_15=regtb_15%>%mutate(manualmodel = map(train, ~glm(formular, data = .x, family = binomial,na.action = na.exclude)))
regtb_17=regtb_17%>%mutate(manualmodel = map(train, ~glm(formular, data = .x, family = binomial,na.action = na.exclude)))



regtb_17=regtb_17%>%mutate(
               ###Psudo R2 analysis
               McFadden_pr2_automodel= map_dbl(.x = automodel, ~psr2glm(.)[["McFadden"]]),
                McFadden_pr2_manualmodel = map_dbl(.x = manualmodel, ~psr2glm(.)[["McFadden"]]),
               AIC_automodel= map_dbl(.x = automodel, ~AIC(.)),
               AIC_manualmodel= map_dbl(.x = manualmodel, ~AIC(.)))

regtb_17


```







```{r QA}


 glm(formula = current_vaping ~ carring_weapon + sad_hopeless + 
    attempted_suicide + safety_concerns_at_school + physical_fighting + 
    bullying_at_school + bullying_electronically + age + race7 + 
    illegal_injected_drug_use + multiple_sex_partner, family = binomial, 
    data = df17)%>%psr2glm()
 
 glm(current_vaping ~ sad_hopeless + attempted_suicide  + safety_concerns_at_school  + physical_fighting + bullying_electronically + carring_weapon  , data = df17, family = binomial())%>%psr2glm()



 formula_1 =c( "carring_weapon" , "sad_hopeless" ,
    "attempted_suicide" , "safety_concerns_at_school" , "physical_fighting" ,
    "bullying_at_school" ,"bullying_electronically" , "age","race7" , 
    "illegal_injected_drug_use" ,"multiple_sex_partner")
 
formula_2=  c("sad_hopeless",
"attempted_suicide",
"safety_concerns_at_school",
"threatened_at_school" ,
"physical_fighting ",
"bullying_electronically",
"carring_weapon")


 formula_1 %in% formula_2
  
```






```{r Fitting the first logistic regression model}
# Fitting a logistic regression model
fit_logistic = glm(current_vaping ~ sad_hopeless + attempted_suicide  + safety_concerns_at_school + threatened_at_school + physical_fighting + bullying_electronically + carring_weapon  , data = df, family = binomial())

fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  knitr::kable(digits = 3)

# What is the contribution of each predictor? see: https://uc-r.github.io/logistic_regression#multi
caret::varImp(fit_logistic)


# Pseudo R^2: see https://uc-r.github.io/logistic_regression#multi (Pseudo R^2 about 0.40 considered good)


pR2(  glm(current_vaping ~ sad_hopeless + attempted_suicide  + safety_concerns_at_school + threatened_at_school + physical_fighting + bullying_electronically + carring_weapon  , data = df17, family = binomial())  )["McFadden"]


# Cross-Validating the model

data_train <- trainControl(method = "cv", number = 5)

model_caret <- train(
  current_vaping ~ sad_hopeless + attempted_suicide  + safety_concerns_at_school + threatened_at_school + physical_fighting + bullying_electronically + carring_weapon,
                   data = df,
                   trControl = data_train,
                   method = 'glm',
                   family = binomial(),
                   na.action = na.pass)
  

?train()

# Model predictions using 4 parts of the data for training 
model_caret

AIC(fit_logistic)
```






